{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObh2iFDI4LQloE9+b8MJh7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArthiyaD/TroubleShooters/blob/Development/ACG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohhq97L1PZUS"
      },
      "outputs": [],
      "source": [
        "!pip install gensim==3.8.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "from IPython.display import Image, display\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing import image, sequence\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ],
      "metadata": {
        "id": "JFzaG53LPdnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Y6v9wfUkibbYBTso534eK9uWJ3JzXKO9\n",
        "!gdown 1Z4C6sVs5EhnwhW6I8m-DKhYJw3e7A6WX\n",
        "!gdown 1Xv3CVAUuQPZbgQJqWJbuiPx14hcApcj5\n",
        "!gdown 1YN4m-gece5itGTzIUdZC1t8pn1sWWZOc\n",
        "!gdown 1jNFcFytMGUnG3oTgIYttilXD5B7ORjlD"
      ],
      "metadata": {
        "id": "5uq6UNTXPiRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vect = '/content/GoogleNews-vectors-negative300.bin.gz'\n",
        "model_text = '/content/model_text_categorize.h5'\n",
        "model_image = '/content/saved_model.hp5'\n",
        "\n",
        "loaded_model = tf.keras.models.load_model(model_text)\n",
        "saved_model = tf.keras.models.load_model(model_image)\n",
        "model = KeyedVectors.load_word2vec_format(word_vect, \n",
        "                                          binary=True,\n",
        "                                          limit=1000000)"
      ],
      "metadata": {
        "id": "VWlMtuTmPlRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = ResNet50(include_top=False, weights='imagenet',\n",
        "                  input_shape=(224,224,3), pooling='avg')"
      ],
      "metadata": {
        "id": "_okk2zPTPmtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/word_2_indices.pickle', 'rb') as handle:\n",
        "     word_2_indices = pkl.load(handle)\n",
        "\n",
        "with open('/content/indices_2_word.pickle', 'rb') as handle:\n",
        "    indices_2_word = pkl.load(handle)"
      ],
      "metadata": {
        "id": "Ec5ZwbPfQ_g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(img_path):\n",
        "    im = image.load_img(img_path, target_size=(224,224,3))\n",
        "    im = image.img_to_array(im)\n",
        "    im = np.expand_dims(im, axis=0)\n",
        "    return im\n",
        "\n",
        "def get_encoding(model1, img):\n",
        "    image = preprocessing(img)\n",
        "    pred = model1.predict(image, verbose=0).reshape(2048)\n",
        "    return pred\n",
        "\n",
        "def predict_captions(image,model):\n",
        "    start_word = [\"<start>\"]\n",
        "    while True:\n",
        "        par_caps = [word_2_indices[i] for i in start_word]\n",
        "        par_caps = sequence.pad_sequences([par_caps], maxlen=40, \n",
        "                                          padding='post')\n",
        "        preds = model.predict([np.array([image]), np.array(par_caps)], verbose=0)\n",
        "        word_pred = indices_2_word[np.argmax(preds[0])]\n",
        "        start_word.append(word_pred)\n",
        "        \n",
        "        if word_pred == \"<end>\" or len(start_word) >40:\n",
        "            break\n",
        "            \n",
        "    return ' '.join(start_word[1:-1])\n",
        "\n",
        "def word_vector(tokens,size,model):\n",
        "  vec = np.zeros(size).reshape((1, size))\n",
        "  count = 0\n",
        "  for word in tokens:\n",
        "    vec += model[word].reshape((1, size))\n",
        "    count += 1.\n",
        "  if count != 0:\n",
        "      vec /= count\n",
        "  return vec\n",
        "\n",
        "def token_check(x,model):\n",
        "  token_list=[]\n",
        "  for i in x:\n",
        "      if len(i) > 3 and i in model.vocab:\n",
        "          token_list.append(i)\n",
        "      else:\n",
        "          continue\n",
        "  return token_list\n",
        "\n",
        "def clean_description_text(description):\n",
        "  description = description.replace(r'\\d+','')\n",
        "  spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
        "                \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
        "                \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
        "                \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\"]\n",
        "  for char in spec_chars:\n",
        "      description = description.replace(char, ' ')\n",
        "\n",
        "  word_list_t = description.lower().split() \n",
        " \n",
        "  filtered_words = [word for word in word_list_t \n",
        "                    if word not in stopwords.words('english')]\n",
        "  text = ' '.join(filtered_words)\n",
        "  return text\n",
        "\n",
        "def FrameCapture(path):\n",
        "    vidObj = cv2.VideoCapture(path)\n",
        "    success, image = vidObj.read()\n",
        "    cv2.imwrite(\"frame.jpg\", image)\n"
      ],
      "metadata": {
        "id": "8Fj6XNsfRBRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0L4YUkdtsDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}